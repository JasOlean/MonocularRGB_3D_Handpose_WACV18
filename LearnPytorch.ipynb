{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearnPytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOQw43ClXgUYn8Kd9YCkbw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasOlean/MonocularRGB_3D_Handpose_WACV18/blob/master/LearnPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S45LKcYLpUcx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create randodm input and output data\n",
        "x = np.linspace(-math.pi, math.pi,2000)\n",
        "print(x)\n",
        "y = np.sin(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okVbaZ6hpXcr",
        "outputId": "65d9254d-b9cf-4124-d790-808e4f552622"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.14159265 -3.13844949 -3.13530633 ...  3.13530633  3.13844949\n",
            "  3.14159265]\n",
            "[-1.22464680e-16 -3.14315906e-03 -6.28628707e-03 ...  6.28628707e-03\n",
            "  3.14315906e-03  1.22464680e-16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly initialize weights\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()"
      ],
      "metadata": {
        "id": "VPmOLk15prbT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  #forward pass: compute predicted y \n",
        "  # y = a + bx+ cx^2 + dx^3\n",
        "  y_pred = a + b*x + c * x**2 + d * x **3\n",
        "\n",
        "  #compute and print loss\n",
        "  loss = np.square(y_pred-y).sum()\n",
        "  if t%100 == 99:\n",
        "    print(t,loss)\n",
        "\n",
        "  #backprop to compute gradients of a,b,c,d with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x**2).sum()\n",
        "  grad_d = (grad_y_pred * x**3).sum()\n",
        "\n",
        "  #update weights\n",
        "  a -= learning_rate * grad_a\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b}x + {c}x^2 + {d}x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzPIV0qzp247",
        "outputId": "31ec1a21-ec3d-4345-86f1-7b90f6c50a72"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 tensor(9.1063)\n",
            "199 tensor(9.0207)\n",
            "299 tensor(8.9605)\n",
            "399 tensor(8.9181)\n",
            "499 tensor(8.8883)\n",
            "599 tensor(8.8673)\n",
            "699 tensor(8.8525)\n",
            "799 tensor(8.8421)\n",
            "899 tensor(8.8347)\n",
            "999 tensor(8.8296)\n",
            "1099 tensor(8.8259)\n",
            "1199 tensor(8.8233)\n",
            "1299 tensor(8.8215)\n",
            "1399 tensor(8.8202)\n",
            "1499 tensor(8.8193)\n",
            "1599 tensor(8.8187)\n",
            "1699 tensor(8.8183)\n",
            "1799 tensor(8.8179)\n",
            "1899 tensor(8.8177)\n",
            "1999 tensor(8.8175)\n",
            "Result: y = -0.000646639964543283 + 0.8566486835479736x + 0.00011155604443047196x^2 + -0.09331728518009186x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Tensor to speed up learning\n",
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "#device = torch.device('cuda:0')  #uncomment this to run on GPU\n",
        "\n",
        "#create random  input and output data\n",
        "x = torch.linspace(-math.pi,math.pi,2000,device = device, dtype = dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#random initialize weights\n",
        "a = torch.randn((), device=device, dtype = dtype)\n",
        "b = torch.randn((), device=device, dtype = dtype)\n",
        "c = torch.randn((), device=device, dtype = dtype)\n",
        "d = torch.randn((), device=device, dtype = dtype)\n",
        "\n",
        "learning_rate =  1e-6\n",
        "for t in range(2000):\n",
        "  #forward pass: compute predicted y \n",
        "  # y = a + bx+ cx^2 + dx^3\n",
        "  y_pred = a + b*x + c * x**2 + d * x **3\n",
        "\n",
        "  #compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  if t%100 == 99:\n",
        "    print(t,loss)\n",
        "  \n",
        "  #backprop to compute gradients of a,b,c,d with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x**2).sum()\n",
        "  grad_d = (grad_y_pred * x**3).sum()\n",
        "\n",
        "  #update weights\n",
        "  a -= learning_rate * grad_a\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
      ],
      "metadata": {
        "id": "K01lEfXZrWOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#auto gradient computation \n",
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "\n",
        "x = torch.linspace(-math.pi,math.pi,2000,dtype=dtype, device = device)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#setting requires_grad=True indicates that we want to compute gradients with\n",
        "#respect to these Tensors during the backward pass.\n",
        "a = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "b = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "c = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "d = torch.randn((),device=device,dtype=dtype,requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  #forward pass: compute predicted y \n",
        "  # y = a + bx+ cx^2 + dx^3\n",
        "  y_pred = a + b*x + c * x**2 + d * x **3\n",
        "\n",
        "  #compute and print loss\n",
        "  loss = (y_pred-y).pow(2).sum()\n",
        "  if t%100 == 99:\n",
        "    print(t,loss.item())\n",
        "  # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
        "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
        "  loss.backward()\n",
        "\n",
        "  #manually update weights\n",
        "  with torch.no_grad():\n",
        "    a -= learning_rate * a.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    c -= learning_rate * c.grad\n",
        "    d -= learning_rate * d.grad\n",
        "\n",
        "    # Manually zero the gradients after updating weights\n",
        "    a.grad = None\n",
        "    b.grad = None\n",
        "    c.grad = None\n",
        "    d.grad = None\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aZ6mBsbuQEG",
        "outputId": "d0cb46ad-3ece-4cf2-b558-010da573879d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 909.4486083984375\n",
            "199 616.0374145507812\n",
            "299 418.60565185546875\n",
            "399 285.63787841796875\n",
            "499 196.00323486328125\n",
            "599 135.52230834960938\n",
            "699 94.67332458496094\n",
            "799 67.05611419677734\n",
            "899 48.36546325683594\n",
            "999 35.702964782714844\n",
            "1099 27.115097045898438\n",
            "1199 21.284469604492188\n",
            "1299 17.321434020996094\n",
            "1399 14.624809265136719\n",
            "1499 12.787801742553711\n",
            "1599 11.534960746765137\n",
            "1699 10.679574012756348\n",
            "1799 10.094866752624512\n",
            "1899 9.694721221923828\n",
            "1999 9.420577049255371\n",
            "Result: y = 0.019878694787621498 + 0.8413488864898682 x + -0.0034294065553694963 x^2 + -0.09114101529121399 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#construction custom autograd function\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class LegendrePolynomial3(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx,input):\n",
        "    ctx.save_for_backward(input)\n",
        "    return 0.5 * (5*input**3 - 3**input)\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx,grad_output):\n",
        "    input, = ctx.saved_tensors\n",
        "    return grad_output * 1.5 * (5*input**2-1)  #differentiate \n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "\n",
        "x = torch.linspace(-math.pi,math.pi,2000,dtype = dtype, device=device)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#create random tensor for weights\n",
        "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "for t in range(2000):\n",
        "\n",
        "  #to apply function, use Function.apply method for 'P3'\n",
        "  P3= LegendrePolynomial3.apply\n",
        "\n",
        "  y_pred = a + b*P3(c+d*x)\n",
        "\n",
        "  loss = (y_pred-y).pow(2).sum()\n",
        "  if t%100 == 99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    a -= learning_rate * a.grad \n",
        "    b -= learning_rate * b.grad\n",
        "    c -= learning_rate * c.grad\n",
        "    d -= learning_rate * d.grad \n",
        "\n",
        "    #manually zero the gradients after updating\n",
        "    a.grad = None\n",
        "    b.grad = None\n",
        "    c.grad = None\n",
        "    d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()}  x)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSUqcb4TxXsu",
        "outputId": "1ce5ba85-bd33-43bc-c825-5fbda82b6101"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 935.1452026367188\n",
            "199 928.3433227539062\n",
            "299 925.6971435546875\n",
            "399 923.1605224609375\n",
            "499 920.6212158203125\n",
            "599 918.07275390625\n",
            "699 915.5157470703125\n",
            "799 912.9503784179688\n",
            "899 910.3770751953125\n",
            "999 907.7962646484375\n",
            "1099 905.2083129882812\n",
            "1199 902.6133422851562\n",
            "1299 900.01220703125\n",
            "1399 897.40478515625\n",
            "1499 894.7915649414062\n",
            "1599 892.1728515625\n",
            "1699 889.549072265625\n",
            "1799 886.920166015625\n",
            "1899 884.286865234375\n",
            "1999 881.6491088867188\n",
            "Result: y = -0.6717701554298401 + -1.2566293478012085 * P3(0.010491049848496914 + 0.20969834923744202  x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import linear\n",
        "# nn module \n",
        "import torch\n",
        "import math\n",
        "\n",
        "#create tensor to hold input and output\n",
        "x = torch.linspace(-math.pi,math.pi,2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "#output y is a linear function(x,x2,x3)\n",
        "p = torch.tensor([1,2,3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "#linear module\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,1),\n",
        "    torch.nn.Flatten(0,1)\n",
        ")\n",
        "\n",
        "#mse loss\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = model(xx)\n",
        "\n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%100 == 99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  #zero the gradients before backward\n",
        "  model.zero_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= learning_rate * param.grad\n",
        "  \n",
        "#first layer access\n",
        "linear_layer = model[0]\n",
        "\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()}x + \\\n",
        "{linear_layer.weight[:,1].item()}x^2 + {linear_layer.weight[:,2].item()}x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEiyNWV33caH",
        "outputId": "28715e74-8797-4043-d3b5-50bdaf4d9d15"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 421.3373718261719\n",
            "199 281.667236328125\n",
            "299 189.2900390625\n",
            "399 128.1907958984375\n",
            "499 87.7784423828125\n",
            "599 61.04828643798828\n",
            "699 43.3677978515625\n",
            "799 31.67279052734375\n",
            "899 23.9367618560791\n",
            "999 18.819528579711914\n",
            "1099 15.434391021728516\n",
            "1199 13.195079803466797\n",
            "1299 11.7136812210083\n",
            "1399 10.733613014221191\n",
            "1499 10.085238456726074\n",
            "1599 9.656248092651367\n",
            "1699 9.372414588928223\n",
            "1799 9.184614181518555\n",
            "1899 9.060344696044922\n",
            "1999 8.978102684020996\n",
            "Result: y = 0.0015562692424282432 + 0.8444920182228088x + -0.0002684828359633684x^2 + -0.0915881022810936x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer \n",
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.linspace(-math.pi,math.pi,2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "p = torch.tensor([1,2,3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "model  = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3,1),\n",
        "    torch.nn.Flatten(0,1)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.RMSprop(model.parameters(),lr = learning_rate)\n",
        "for t in range(2000):\n",
        "  y_pred = model(xx)\n",
        "\n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%100 == 99:\n",
        "    print(t,loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  #calling the step fun on an Optimizer makes an update to its parameter\n",
        "  optimizer.step()\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()} x + \\\n",
        "{linear_layer.weight[:,1].item()}x^2 + {linear_layer.weight[:,2].item()}x^3')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFnfauV-39vE",
        "outputId": "758e9709-7da8-41a4-d6c8-72f34c4b8478"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 2579.18994140625\n",
            "199 1319.304443359375\n",
            "299 795.3138427734375\n",
            "399 528.42333984375\n",
            "499 350.76837158203125\n",
            "599 230.48455810546875\n",
            "699 148.92584228515625\n",
            "799 93.09563446044922\n",
            "899 55.15594482421875\n",
            "999 30.691818237304688\n",
            "1099 16.858619689941406\n",
            "1199 10.768710136413574\n",
            "1299 9.088170051574707\n",
            "1399 8.863669395446777\n",
            "1499 8.874008178710938\n",
            "1599 8.923108100891113\n",
            "1699 8.921326637268066\n",
            "1799 8.903042793273926\n",
            "1899 8.907391548156738\n",
            "1999 8.922664642333984\n",
            "Result: y = -0.00047350150998681784 + 0.8572486042976379 x + -0.0005153247620910406x^2 + -0.0928226038813591x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#custom nn modules\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate four parameters and assign them as\n",
        "    member parameters.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.a = torch.nn.Parameter(torch.randn(()))\n",
        "    self.b = torch.nn.Parameter(torch.randn(()))\n",
        "    self.c = torch.nn.Parameter(torch.randn(()))\n",
        "    self.d = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "  def forward(self,x):\n",
        "      return self.a + self.b *x + self.c * x**2 + self.d * x**3\n",
        "    \n",
        "  def string(self):\n",
        "      return f'y = {self.a.item()} + {self.b.item()}x + {self.c.item()}x^2\\\n",
        "      + {self.d.item()}x^3'\n",
        "\n",
        "#create tensor to hold input and output\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "model = Polynomial3()\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "optim = torch.optim.SGD(model.parameters(),lr=1e-6)\n",
        "for t in range(2000):\n",
        "  y_pred = model(x)\n",
        "\n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%100 == 99:\n",
        "    print(t,loss.item())\n",
        "\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "print(f'Result: {model.string()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sglOt9UP9_Sx",
        "outputId": "49492718-fd88-49d5-a9c5-6dce03eca68d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 406.01812744140625\n",
            "199 275.95001220703125\n",
            "299 188.6317596435547\n",
            "399 129.9646453857422\n",
            "499 90.51476287841797\n",
            "599 63.96409225463867\n",
            "699 46.07876968383789\n",
            "799 34.01948928833008\n",
            "899 25.880661010742188\n",
            "999 20.38235855102539\n",
            "1099 16.664201736450195\n",
            "1199 14.147226333618164\n",
            "1299 12.441591262817383\n",
            "1399 11.284544944763184\n",
            "1499 10.498756408691406\n",
            "1599 9.964524269104004\n",
            "1699 9.600916862487793\n",
            "1799 9.353153228759766\n",
            "1899 9.184141159057617\n",
            "1999 9.068706512451172\n",
            "Result: y = 0.01231743860989809 + 0.8462655544281006x + -0.002124962629750371x^2      + -0.09184036403894424x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#control flow + weight sharing\n",
        "#compute fourth and fifth order\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import random \n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "      \"\"\"\n",
        "      In the constructor we instantiate five parameters and assign them as members.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.a = torch.nn.Parameter(torch.randn(()))\n",
        "      self.b = torch.nn.Parameter(torch.randn(()))\n",
        "      self.c = torch.nn.Parameter(torch.randn(()))\n",
        "      self.d = torch.nn.Parameter(torch.randn(()))\n",
        "      self.e = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"\n",
        "      For the forward pass of the model, we randomly choose either 4, 5\n",
        "      and reuse the e parameter to compute the contribution of these orders.\n",
        "\n",
        "      Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "      Python control-flow operators like loops or conditional statements when\n",
        "      defining the forward pass of the model.\n",
        "\n",
        "      Here we also see that it is perfectly safe to reuse the same parameter many\n",
        "      times when defining a computational graph.\n",
        "      \"\"\"\n",
        "      y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "      for exp in range(4, random.randint(4, 6)):\n",
        "          y = y + self.e * x ** exp\n",
        "      return y\n",
        "  \n",
        "  def string(self):\n",
        "\n",
        "    return f'y = {self.a.item()} + {self.b.item()}x + {self.c.item()}x^2+\\\n",
        "    {self.d.item()}x^3 + {self.e.item()}x^4 ? +  {self.e.item()}.x^5 ?'\n",
        "\n",
        "#create data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "print(x)\n",
        "y = torch.sin(x)\n",
        "\n",
        "model = DynamicNet()\n",
        "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
        "optim = torch.optim.SGD(model.parameters(),lr=1e-8,momentum = 0.9)\n",
        "\n",
        "for t in range(30000):\n",
        "  y_pred = model(x)\n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%2000 == 1999:\n",
        "    print(t,loss.item())\n",
        "  \n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4hcN_3hJpaA",
        "outputId": "f795b28e-58fa-4586-91be-efba46536a3f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-3.1416, -3.1384, -3.1353,  ...,  3.1353,  3.1384,  3.1416])\n",
            "1999 1919.784423828125\n",
            "3999 922.7003173828125\n",
            "5999 435.98876953125\n",
            "7999 198.5419921875\n",
            "9999 95.75335693359375\n",
            "11999 47.299861907958984\n",
            "13999 27.01592445373535\n",
            "15999 17.344894409179688\n",
            "17999 12.730238914489746\n",
            "19999 10.644499778747559\n",
            "21999 9.68739128112793\n",
            "23999 9.119975090026855\n",
            "25999 8.819015502929688\n",
            "27999 8.723674774169922\n",
            "29999 8.871621131896973\n",
            "Result: y = -0.0045758686028420925 + 0.8532346487045288x + 0.0003041607851628214x^2+    -0.09305181354284286x^3 + 9.429931378690526e-05x^4 ? +  9.429931378690526e-05.x^5 ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XWBv2rLOMBJe"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}